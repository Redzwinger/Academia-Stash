# -*- coding: utf-8 -*-
"""R030_Deep_Learning_Lab01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17AiITeJGxbrfnFsElBLYwDSu9hKqvOMK

---

*Name: Achintya Kamath*

*Roll Number: R030*

*MBA Tech Artificial Intelligence*

*Subject: Deep Learning*

*Lab 01 - Tensorflow 2.0 [Basics]*

*Date: 19-07-2023*

---
"""

'''
Name: Achintya Kamath
Roll Number: R030
MBA Tech Artificial Intelligence
Subject: Deep Learning
Lab 01 - Tensorflow 2.0 [Basics]
Date: 19-07-2023
'''

"""# ***TASK 1***

*Perform basic operations using Tensorflow*

*(Defining constants, variables, concatenation, add, multiply, reduce mean, reduce sum)*
"""

import tensorflow as tf
import numpy as np
print("This program utilises Tensorflow verison", tf.__version__, " ")

#Constants

const = tf.constant([[7,8,3], [5,9,1]])
print(const)

print(const.shape)

print(const.dtype)

print(const.numpy())

#Variables

varb = tf.Variable([0.0, 0.0, 0.0])
varb.assign([1,5,6])

print(varb)

print(varb.numpy())

varb.assign_add([1,1,1])
print(varb.numpy())

#Concatenation (columns and rows)

A = tf.constant([[3,4,5], [1,2,3]])
B = tf.Variable([[1,1,1], [2,3,4]])

AB_concat_col = tf.concat([A,B], axis=1)
print(AB_concat_col.numpy())

AB_concat_row = tf.concat([A,B], axis=0)
print(AB_concat_row.numpy())

reshaped_concat = tf.reshape(AB_concat_row, shape = [1,12])
print(reshaped_concat.numpy())

#Addition

A = tf.constant([[3,4,5], [1,2,3]])
B = tf.constant([[1,1,1], [2,3,4]])
C = tf.constant([[5,1,5], [2,2,3]])

print(A.numpy())
print("\n", B.numpy())

A_plus_B = tf.add(A,B)
print(A_plus_B.numpy())

print(C.numpy())

APB_plus_C = tf.add(A_plus_B, C)
print(APB_plus_C.numpy())

variation_APBPC = tf.add(A, B, C)
print(variation_APBPC.numpy())

#Multiplication

A_multi_B = tf.multiply(A,B)
print(A.numpy(), "\n\n", B.numpy(), '\n')
print(A_multi_B.numpy())

AMB_multi_C = tf.multiply(A_multi_B,C)
print(C.numpy(), '\n')
print(AMB_multi_C.numpy())

#Reducing Mean

G = tf.constant([[1.55, 2.54, 8.35, 9.25], [1.00, 2.00, 8.95, 4.00]])
print(G.numpy())

reduced_G = tf.reduce_mean(G, 0)
print(reduced_G.numpy())

#Reducing Sum

S = tf.constant([[5.55, 3.54, 7.35, 9.25], [5.00, 6.00, 8.95, 4.00]])
SS = tf.constant([[1.55, 2.54, 8.35, 9.25], [1.00, 2.00, 8.95, 4.00]])

print(S.numpy())

print(SS.numpy())

RS_3 = tf.reduce_sum([S, SS])
print(RS_3.numpy())

"""# ***TASK 2***

*Perform linear algebra operations using Tensorflow*

*(Transpose of a matrix, matrix multiplication, element-wise multiplication, determinant of a matrix)*
"""

#Transpose of a Matrix

a_const = tf.constant([[2,7], [5,9]])
print(a_const.numpy())

a_trans = tf.transpose(a_const)
print(a_trans.numpy())

#Matrix Multiplication

b_const = tf.constant([[3,8], [7,2]])
print(b_const.numpy())

a_mul_b = tf.matmul(a_const, b_const)
print(a_mul_b.numpy())

#Element-Wise Multiplication

b_const = tf.constant([[3,8], [7,2]])
mu = tf.constant([[10,6]])
print(mu.numpy())

ele_mul = tf.multiply(b_const,mu)
print(ele_mul.numpy())

#Determinant

ele_mul = tf.cast(ele_mul, tf.float32)
det = tf.linalg.det(ele_mul)
print(det.numpy())

"""# ***Task 3***

*Perform derivative and higher order derivative for function,*

*f(x) = x^3*

*using gradient tape of Tensorflow*
"""

#using concept f(x) = x^3
const_x = tf.constant(4.0)
with tf.GradientTape() as tape:
    tape.watch(const_x)
    y = const_x**3

grad = tape.gradient(y, const_x)
print("The Derivative is =", grad.numpy())

#Going for higher order now...
new_x = tf.Variable(4.0, trainable=True)
with tf.GradientTape() as tapeOne:
    with tf.GradientTape() as tapeTwo:
        y = new_x**3
        order = tapeTwo.gradient(y, new_x)  # First Order

Higher_Order = tapeOne.gradient(order, new_x)  # Second Order
print("The Higher Order Derivative is =", Higher_Order.numpy())


"""# ***Task 4***

*Compute WX+b using Tensorflow where W, X, and b are drawn from a random normal distribution.*

*W is of shape (4, 3), X is (3,1) and b is (4,1)*
"""

#Compute WX+b using Tensorflow where W, X, and b are drawn from a
#random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1)

#Using np.random.randn to randomly assign constants

w_mat = tf.constant(np.random.randn(4,3))
x_mat = tf.constant(np.random.randn(3,1))
b_mat = tf.constant(np.random.randn(4,1))

print(w_mat.numpy(), "\n")
print(x_mat.numpy(), "\n")
print(b_mat.numpy())

WX = tf.matmul(w_mat,x_mat)
print(WX.numpy())

WX_plus_b = tf.add(WX, b_mat)
print(WX_plus_b.numpy())

"""# ***Task 5***

*Compute Gradient of sigmoid function using Tensor flow*
"""

#Formula: f(x) = 1/1+exp(-x)

def sigma_gradient(someThing):
    with tf.GradientTape() as tape_sig:
        f = (1 / 1 + (tf.math.exp(-someThing)))
    sigma_grad = tape_sig.gradient(f, someThing)
    return sigma_grad.numpy()

variable_but_constant = tf.Variable(4.0, trainable=True)

print("Gradient of the sigmoid", variable_but_constant.numpy(), " is ", sigma_gradient(variable_but_constant))

"""# ***Task 6***

*Identify two research paper based on deep learning.*

*State for which application they have used deep learning.*

*(Cite the papers)*
"""

#Maybe do it? I don't know :P